{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"assignment_2_data\"\n",
    "anime = pd.read_csv(f'{path}/assignment_2_anime.csv')\n",
    "rtrain = pd.read_csv(f\"{path}/assignment_2_ratings_train.csv\")\n",
    "rtest = pd.read_csv(f\"{path}/assignment_2_ratings_test.csv\")\n",
    "\n",
    "# preprocess rtrain as discussed in explore.ipynb\n",
    "rtrain = rtrain.drop_duplicates(subset=['user_id', 'anime_id'], keep='last')\n",
    "rtest = rtest.drop_duplicates(subset=['user_id', 'anime_id'], keep='last')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build similiarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = pd.get_dummies(anime['type'])\n",
    "from scipy.sparse import csr_matrix\n",
    "sparse_one = csr_matrix(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "vectorizer1 = TfidfVectorizer()\n",
    "vectorizer2_name = TfidfVectorizer()\n",
    "\n",
    "\n",
    "name_matrix = vectorizer2_name.fit_transform(anime['name'].values)\n",
    "# use svd to get the dimension reduction\n",
    "k = 100\n",
    "svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "reduced_matrix= svd.fit_transform(name_matrix)\n",
    "# use another vectorizer to calculate genre\n",
    "anime['genre'] = anime['genre'].fillna('')\n",
    "genre_matrix = vectorizer1.fit_transform(anime['genre']) # 47 genre \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "feature_matrix = hstack((reduced_matrix,genre_matrix, onehot.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1 = np.full((k,), 0.7)\n",
    "weights_2 = np.full((47,), 0.2)\n",
    "weights_3 = np.full((6,), 0.1)\n",
    "weights = np.concatenate((weights_1, weights_2,weights_3))\n",
    "weighted_matrix = feature_matrix.multiply(weights)\n",
    "\n",
    "# the weighted of name genre type are 0.1 0.8 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(weighted_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_matrix = cosine_similarity(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_from_anime_id(anime_id, anime_df):\n",
    "    index = anime_df[anime_df['anime_id'] == anime_id].index[0]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(animes_col,anime):\n",
    "    ls = []\n",
    "    for anime_id in animes_col:\n",
    "        ls.append(get_index_from_anime_id(anime_id,anime))\n",
    "    return ls\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_prediction(user_id, anime_id, user_ratings, anime_df, similarities):\n",
    "    \"given the anime_id, user_id in the test dataset\"\n",
    "    # Get the index of the anime in the anime_df\n",
    "    anime_index = anime_df[anime_df['anime_id'] == anime_id].index[0]\n",
    "\n",
    "    # Get the ratings of the user, pandaframe\n",
    "    user_rated_animes = user_ratings[user_ratings['user_id'] == user_id]\n",
    "\n",
    "    # Calculate the weighted sum of the user's ratings and the genre similarity\n",
    "    #change the index\n",
    "    weighted_sum = np.sum(user_rated_animes['rating'].values * similarities[anime_index][user_rated_animes['index'].values])\n",
    "\n",
    "    # Normalize the weighted sum by the sum of genre similarities\n",
    "    prediction = weighted_sum / np.sum(similarities[anime_index][user_rated_animes['index'].values])\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = anime.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = pd.merge(rtrain, anime[['index','anime_id']], on='anime_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44017</td>\n",
       "      <td>13161</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  anime_id  rating\n",
       "0    44017     13161       4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtest.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.533738349380632"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_based_prediction(44017, 13161, index_train, anime, similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create a new DataFrame for storing the results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[ \u001b[39m'\u001b[39m\u001b[39mGround_Truth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent_pred\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[39m# Calculate the combined predictions for each (user_id, anime_id) pair in the test dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m user_id, anime_id, true_rating \u001b[39min\u001b[39;00m rtest\u001b[39m.\u001b[39mvalues: \n\u001b[1;32m      7\u001b[0m     \u001b[39m# use rtest.head(5000 as example)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \n\u001b[1;32m      9\u001b[0m     \u001b[39m#we use 0 first, \u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m### ??? if \u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39m#collab_pred = algo.predict(user_id, anime_id).est\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame for storing the results\n",
    "test_results = pd.DataFrame(columns=[ 'Ground_Truth', 'content_pred'])\n",
    "\n",
    "\n",
    "# Calculate the combined predictions for each (user_id, anime_id) pair in the test dataset\n",
    "for user_id, anime_id, true_rating in rtest.values: \n",
    "    # use rtest.head(5000 as example)\n",
    "    \n",
    "    #we use 0 first, \n",
    "    ### ??? if \n",
    "    #collab_pred = algo.predict(user_id, anime_id).est\n",
    "    content_pred = content_based_prediction(user_id, anime_id, index_train, anime, similarity_matrix)\n",
    "    \n",
    "  \n",
    "    test_results = test_results.append({\n",
    "        'Ground_Truth':true_rating,\n",
    "        'content_pred': content_pred,\n",
    "        }, \n",
    "        ignore_index = True\n",
    "    )\n",
    "\n",
    "print(test_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.7582105691605658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "# Replace any NaN values in the 'Ground_Truth' and 'content_pred' columns with zeros\n",
    "ground_truth = np.nan_to_num(test_results['Ground_Truth'], nan=0.0)\n",
    "content_pred = np.nan_to_num(test_results['content_pred'], nan=0.0)\n",
    "\n",
    "# Calculate the MSE between the modified 'Ground_Truth' and 'content_pred' columns\n",
    "mse = mean_squared_error(ground_truth, content_pred)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different weightage\n",
    "0.9 0.05  0.05  MSE 2.1\n",
    "0.7 0.2 0.1 MSE: 1.76\n",
    "0.4 0.4 0.1 MSE: 1.76\n",
    "            \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "366bd0e816929564b8da31f5ec5d54b0e804cb407beb456598d4b1a5e6c495cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
