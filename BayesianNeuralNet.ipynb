{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhFFMVOAUTQBjOTONw3YeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BharathSShankar/DSA4212_Assignments/blob/bharath-exp/BayesianNeuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist\n",
        "!python -m medmnist download"
      ],
      "metadata": {
        "id": "39qHekF2tOcn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a8aea2-0ce9-4089-85cd-69e638740dc9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-2.2.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from medmnist) (2.0.0+cu118)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from medmnist) (8.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from medmnist) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from medmnist) (0.15.1+cu118)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from medmnist) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from medmnist) (1.22.4)\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->medmnist) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->medmnist) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (1.10.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (23.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (2023.3.21)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (3.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->medmnist) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->medmnist) (3.1.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->medmnist) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->medmnist) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->medmnist) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->medmnist) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=6da8b7bbc87c339f612ba3df53897bb5fce68a006b7aba31caac2042ddc877ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, medmnist\n",
            "Successfully installed fire-0.5.0 medmnist-2.2.1\n",
            "Downloading pathmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/pathmnist.npz?download=1 to /root/.medmnist/pathmnist.npz\n",
            "100% 205615438/205615438 [00:09<00:00, 22155627.05it/s]\n",
            "Downloading chestmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/chestmnist.npz?download=1 to /root/.medmnist/chestmnist.npz\n",
            "100% 82802576/82802576 [00:04<00:00, 18635539.16it/s]\n",
            "Downloading dermamnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/dermamnist.npz?download=1 to /root/.medmnist/dermamnist.npz\n",
            "100% 19725078/19725078 [00:01<00:00, 13296302.84it/s]\n",
            "Downloading octmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/octmnist.npz?download=1 to /root/.medmnist/octmnist.npz\n",
            "100% 54938180/54938180 [00:03<00:00, 17917042.47it/s]\n",
            "Downloading pneumoniamnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/pneumoniamnist.npz?download=1 to /root/.medmnist/pneumoniamnist.npz\n",
            "100% 4170669/4170669 [00:00<00:00, 4849779.74it/s]\n",
            "Downloading retinamnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/retinamnist.npz?download=1 to /root/.medmnist/retinamnist.npz\n",
            "100% 3291041/3291041 [00:00<00:00, 3325725.47it/s]\n",
            "Downloading breastmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/breastmnist.npz?download=1 to /root/.medmnist/breastmnist.npz\n",
            "100% 559580/559580 [00:00<00:00, 914922.41it/s] \n",
            "Downloading bloodmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/bloodmnist.npz?download=1 to /root/.medmnist/bloodmnist.npz\n",
            "100% 35461855/35461855 [00:02<00:00, 16144095.98it/s]\n",
            "Downloading tissuemnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/tissuemnist.npz?download=1 to /root/.medmnist/tissuemnist.npz\n",
            "100% 124962739/124962739 [00:06<00:00, 20692264.01it/s]\n",
            "Downloading organamnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/organamnist.npz?download=1 to /root/.medmnist/organamnist.npz\n",
            "100% 38247903/38247903 [00:02<00:00, 15731422.15it/s]\n",
            "Downloading organcmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/organcmnist.npz?download=1 to /root/.medmnist/organcmnist.npz\n",
            "100% 15527535/15527535 [00:01<00:00, 10508034.75it/s]\n",
            "Downloading organsmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/organsmnist.npz?download=1 to /root/.medmnist/organsmnist.npz\n",
            "100% 16528536/16528536 [00:01<00:00, 12241110.37it/s]\n",
            "Downloading organmnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/organmnist3d.npz?download=1 to /root/.medmnist/organmnist3d.npz\n",
            "100% 32657407/32657407 [00:02<00:00, 14717605.66it/s]\n",
            "Downloading nodulemnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/nodulemnist3d.npz?download=1 to /root/.medmnist/nodulemnist3d.npz\n",
            "100% 29299364/29299364 [00:02<00:00, 12578366.52it/s]\n",
            "Downloading adrenalmnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/adrenalmnist3d.npz?download=1 to /root/.medmnist/adrenalmnist3d.npz\n",
            "100% 276833/276833 [00:00<00:00, 754076.53it/s]\n",
            "Downloading fracturemnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/fracturemnist3d.npz?download=1 to /root/.medmnist/fracturemnist3d.npz\n",
            "100% 3278419/3278419 [00:00<00:00, 3826054.25it/s]\n",
            "Downloading vesselmnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/vesselmnist3d.npz?download=1 to /root/.medmnist/vesselmnist3d.npz\n",
            "100% 398373/398373 [00:00<00:00, 814779.81it/s]\n",
            "Downloading synapsemnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/synapsemnist3d.npz?download=1 to /root/.medmnist/synapsemnist3d.npz\n",
            "100% 38034583/38034583 [00:02<00:00, 15840690.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm, trange"
      ],
      "metadata": {
        "id": "lmmUvxZDmRz_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOLC37ibgyLS",
        "outputId": "2709405d-03a0-40aa-f351-bce431d265c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cd \"/content/drive/MyDrive/DSA4212/Assignment 3\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import medmnist\n",
        "from medmnist import INFO, Evaluator"
      ],
      "metadata": {
        "id": "5FJe6S9YmRwg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_flag = 'pathmnist'\n",
        "# data_flag = 'breastmnist'\n",
        "download = True\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 128\n",
        "lr = 0.001\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])\n"
      ],
      "metadata": {
        "id": "0id9rD1omRqQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
        "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
        "\n",
        "pil_dataset = DataClass(split='train', download=download)\n",
        "\n",
        "# encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "VhsA3s83hYWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5c3f15-dd09-41e0-ce4d-761674ba473b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PI = 0.5\n",
        "SIGMA_1 = torch.FloatTensor([np.exp(1)]).to(DEVICE)\n",
        "SIGMA_2 = torch.FloatTensor([np.exp(-4)]).to(DEVICE)"
      ],
      "metadata": {
        "id": "IkukrBGf17Sx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Gaussian(object):\n",
        "    def __init__(self, mu, rho):\n",
        "        super().__init__()\n",
        "        self.mu = mu\n",
        "        self.rho = rho\n",
        "        self.normal = torch.distributions.Normal(0,1)\n",
        "    \n",
        "    @property\n",
        "    def sigma(self):\n",
        "        return torch.log1p(torch.exp(self.rho))\n",
        "    \n",
        "    def sample(self):\n",
        "        epsilon = self.normal.sample(self.rho.size()).to(DEVICE)\n",
        "        return self.mu + self.sigma * epsilon\n",
        "    \n",
        "    def log_prob(self, input):\n",
        "        return (-np.log(np.sqrt(2 * np.pi))\n",
        "                - torch.log(self.sigma)\n",
        "                - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"
      ],
      "metadata": {
        "id": "KThYGJYb16lN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaleMixtureGaussian(object):\n",
        "    def __init__(self, pi, sigma1, sigma2):\n",
        "        super().__init__()\n",
        "        self.pi = pi\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        self.gaussian1 = torch.distributions.Normal(0,sigma1)\n",
        "        self.gaussian2 = torch.distributions.Normal(0,sigma2)\n",
        "    \n",
        "    def log_prob(self, input):\n",
        "        prob1 = torch.exp(self.gaussian1.log_prob(input))\n",
        "        prob2 = torch.exp(self.gaussian2.log_prob(input))\n",
        "        return (torch.log(self.pi * prob1 + (1-self.pi) * prob2)).sum()"
      ],
      "metadata": {
        "id": "Gik9nYhW2TvA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianDense(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.1, 0.1))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-8,-6))\n",
        "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.1, 0.1))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-8,-6))\n",
        "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
        "\n",
        "        self.weight_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.bias_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.log_prior = 0\n",
        "        self.log_variational_posterior = 0\n",
        "\n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        if self.training or sample:\n",
        "            weight = self.weight.sample()\n",
        "            bias = self.bias.sample()\n",
        "        else:\n",
        "            weight = self.weight.mu\n",
        "            bias = self.bias.mu\n",
        "        if self.training or calculate_log_probs:\n",
        "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
        "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
        "        else:\n",
        "            self.log_prior, self.log_variational_posterior = 0, 0\n",
        "\n",
        "        return F.linear(input, weight, bias)"
      ],
      "metadata": {
        "id": "h6JucxETUk-4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, k_size):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.k_size = k_size\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_channels, in_channels, k_size, k_size).uniform_(-0.1, 0.1))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_channels, in_channels, k_size,k_size).uniform_(-8,-6))\n",
        "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_channels).uniform_(-0.1, 0.1))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_channels).uniform_(-8,-6))\n",
        "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
        "\n",
        "        self.weight_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.bias_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.log_prior = 0\n",
        "        self.log_variational_posterior = 0\n",
        "\n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        if self.training or sample:\n",
        "            weight = self.weight.sample()\n",
        "            bias = self.bias.sample()\n",
        "        else:\n",
        "            weight = self.weight.mu\n",
        "            bias = self.bias.mu\n",
        "        if self.training or calculate_log_probs:\n",
        "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
        "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
        "        else:\n",
        "            self.log_prior, self.log_variational_posterior = 0, 0\n",
        "\n",
        "        return F.conv2d(input, weight, bias)"
      ],
      "metadata": {
        "id": "Teuk8UqS3Q80"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianNeuralNetFC(nn.Module):\n",
        "    def __init__(self, layers_dims, input_size, n_classes):\n",
        "        super().__init__()\n",
        "        self.inputLayer = BayesianDense(input_size, layers_dims[0])\n",
        "        layer_list = []\n",
        "        for i in range(1, len(layers_dims)):\n",
        "            layer_list.append(BayesianDense(layers_dims[i - 1], layers_dims[i]))\n",
        "        self.linears = nn.ModuleList(layer_list)\n",
        "        self.outputLayer = BayesianDense(layers_dims[-1], n_classes)\n",
        "    \n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        x = nn.Flatten()(input)\n",
        "        x = self.inputLayer(x, sample, calculate_log_probs)\n",
        "        x = F.relu(x)\n",
        "        x = nn.Dropout(p = 0.1)(x)\n",
        "        for layer in self.linears:\n",
        "            x = layer(x, sample, calculate_log_probs)\n",
        "            x = F.relu(x)\n",
        "            x = nn.Dropout(p = 0.1)(x)\n",
        "        x = self.outputLayer(x)\n",
        "        x = F.softmax(x, dim = 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QwlML0Db4kPg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bayesian_net(net, train_loader, test_loader, n_epochs=20, lr=3e-4, log_interval=10):\n",
        "    # Define loss function and optimizer\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    # Move model to device\n",
        "    net.to(DEVICE)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training mode\n",
        "        net.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}\") as t:\n",
        "            for batch_idx, (data, target) in enumerate(t):\n",
        "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                outputs = net(data)\n",
        "\n",
        "                # Compute loss and update model\n",
        "                loss = loss_func(outputs, target.T[0])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update training statistics\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += data.shape[0]\n",
        "                correct += predicted.eq(target.T[0]).sum().item()\n",
        "\n",
        "                # Log training progress\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    train_acc = correct / total\n",
        "                    train_loss /= log_interval\n",
        "                    t.set_postfix(loss=f\"{train_loss:.6f}\", accuracy=f\"{train_acc:.2f}\")\n",
        "                    train_loss = 0\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "\n",
        "        # Evaluation mode\n",
        "        net.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = net(data)\n",
        "\n",
        "                # Compute loss and update evaluation statistics\n",
        "                loss = loss_func(outputs, target.T[0])\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += data.shape[0]\n",
        "                correct += predicted.eq(target.T[0]).sum().item()\n",
        "\n",
        "        # Log evaluation statistics\n",
        "        test_acc = 100. * correct / total\n",
        "        test_loss /= len(test_loader)\n",
        "        print('Test set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n",
        "            test_loss, test_acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "raikcJBO56ad"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = BayesianNeuralNetFC([128, 256, 64], 3 * 28 * 28, 9)\n",
        "train_bayesian_net(net, train_loader, test_loader, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "BVvMsef_pUZf",
        "outputId": "31e25cbb-8518-4e7e-aaaa-1009d0e93dde"
      },
      "execution_count": 22,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 704/704 [00:37<00:00, 18.79it/s, accuracy=0.47, loss=1.903054]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 1.7715, Accuracy: 59.90%\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 704/704 [00:35<00:00, 19.81it/s, accuracy=0.49, loss=1.872848]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 1.7963, Accuracy: 57.21%\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 704/704 [00:35<00:00, 19.65it/s, accuracy=0.51, loss=1.849811]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 1.7782, Accuracy: 58.84%\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 704/704 [00:35<00:00, 19.65it/s, accuracy=0.53, loss=1.840323]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 1.8137, Accuracy: 55.45%\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5:  93%|█████████▎| 655/704 [00:33<00:02, 19.78it/s, accuracy=0.54, loss=1.828342]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1d985f99a89f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianNeuralNetFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_bayesian_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-9d584dc9e398>\u001b[0m in \u001b[0;36mtrain_bayesian_net\u001b[0;34m(net, train_loader, test_loader, n_epochs, lr, log_interval)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;31m# Compute loss and update model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-9bb86bed15ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, sample, calculate_log_probs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_log_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-ce99f7da83fd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, sample, calculate_log_probs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcalculate_log_probs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_variational_posterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_variational_posterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-7a054edc45f9>\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     17\u001b[0m         return (-np.log(np.sqrt(2 * np.pi))\n\u001b[1;32m     18\u001b[0m                 \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-7a054edc45f9>\u001b[0m in \u001b[0;36msigma\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net= nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.LazyLinear(128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.LazyLinear(256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.LazyLinear(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.LazyLinear(9),\n",
        "    nn.Softmax(dim = 1)\n",
        ").to(DEVICE)\n",
        "train_bayesian_net(net, train_loader, test_loader, 10)"
      ],
      "metadata": {
        "id": "AAPJIjrhC5es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianNeuralNetConv(nn.Module):\n",
        "    def __init__(self, channel_list, input_channels, n_classes):\n",
        "        super().__init__()\n",
        "        self.inputLayer = BayesianConv2D(input_channels, channel_list[0], k_size=3)\n",
        "        layer_list = []\n",
        "        for i in range(1, len(channel_list)):\n",
        "            layer_list.append(BayesianConv2D(channel_list[i - 1], channel_list[i], k_size=1))\n",
        "        self.convs = nn.ModuleList(layer_list)\n",
        "        self.fc = BayesianDense(channel_list[-1] * 9, n_classes)\n",
        "    \n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        x = self.inputLayer(input, sample, calculate_log_probs)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, sample, calculate_log_probs)\n",
        "            x = F.relu(x)\n",
        "            x = F.max_pool2d(x, 2)\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.fc(x, sample, calculate_log_probs)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "x4efws5kEV6Q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = BayesianNeuralNetConv([128, 256, 512], 3, 9)\n",
        "train_bayesian_net(net, train_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLfaDJqqyrQL",
        "outputId": "2253c22c-7576-4726-8cbd-f97edc45f9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 704/704 [00:46<00:00, 15.23it/s, accuracy=0.60, loss=1.778098]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 1.7125, Accuracy: 66.63%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 704/704 [00:45<00:00, 15.38it/s, accuracy=0.65, loss=1.728691]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 1.6676, Accuracy: 71.32%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 704/704 [00:44<00:00, 15.77it/s, accuracy=0.75, loss=1.633482]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 1.6212, Accuracy: 76.74%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 704/704 [00:44<00:00, 15.83it/s, accuracy=0.77, loss=1.608163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 1.6162, Accuracy: 76.55%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 704/704 [00:44<00:00, 15.73it/s, accuracy=0.80, loss=1.577386]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 1.6046, Accuracy: 78.26%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 704/704 [00:44<00:00, 15.85it/s, accuracy=0.83, loss=1.551932]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nFLiOK420lzv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}