{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For the genre_vector.\n",
    "genre_map = ['Comedy',\n",
    " 'Action',\n",
    " 'Adventure',\n",
    " 'Fantasy',\n",
    " 'Sci-Fi',\n",
    " 'Drama',\n",
    " 'Shounen',\n",
    " 'Kids',\n",
    " 'Romance',\n",
    " 'School',\n",
    " 'Slice of Life',\n",
    " 'Hentai',\n",
    " 'Supernatural',\n",
    " 'Mecha',\n",
    " 'Music',\n",
    " 'Historical',\n",
    " 'Magic',\n",
    " 'Ecchi',\n",
    " 'Shoujo',\n",
    " 'Seinen',\n",
    " 'Sports',\n",
    " 'Mystery',\n",
    " 'Super Power',\n",
    " 'Military',\n",
    " 'Parody',\n",
    " 'Space',\n",
    " 'Horror',\n",
    " 'Harem',\n",
    " 'Demons',\n",
    " 'Martial Arts',\n",
    " 'Dementia',\n",
    " 'Psychological',\n",
    " 'Police',\n",
    " 'Game',\n",
    " 'Samurai',\n",
    " 'Vampire',\n",
    " 'Thriller',\n",
    " 'Cars',\n",
    " 'Shounen Ai',\n",
    " 'NaN',\n",
    " 'Shoujo Ai',\n",
    " 'Josei',\n",
    " 'Yuri',\n",
    " 'Yaoi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = pd.read_csv('assignment_2_anime.csv')\n",
    "rtrain = pd.read_csv('assignment_2_ratings_train.csv')\n",
    "rtest = pd.read_csv('assignment_2_ratings_test.csv')\n",
    "\n",
    "# preprocess rtrain as discussed in explore.ipynb\n",
    "rtrain = rtrain.drop_duplicates(subset=['user_id', 'anime_id'], keep='last')\n",
    "rtest = rtest.drop_duplicates(subset=['user_id', 'anime_id'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = rtrain.user_id.unique()\n",
    "test_users = rtest.user_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 68421 unique users in the training set.\n",
      "There are 64627 unique users in the test set.\n",
      "63448 users are in both sets.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {train_users.size} unique users in the training set.')\n",
    "print(f'There are {test_users.size} unique users in the test set.')\n",
    "print(f'{np.intersect1d(train_users, test_users).size} users are in both sets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to store the mean rating for each user and anime\n",
    "user_mean = rtrain.groupby('user_id')[['rating']].mean().to_dict()['rating']\n",
    "anime_mean = rtrain.groupby('anime_id')[['rating']].mean().to_dict()['rating']\n",
    "all_mean: float = rtrain.rating.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8638812628457382"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using user_mean\n",
    "MSE = 0\n",
    "for row in rtest.itertuples():\n",
    "    try:\n",
    "        user_mean_pred = user_mean[row.user_id]\n",
    "    except KeyError:\n",
    "        user_mean_pred = all_mean\n",
    "    MSE += (row.rating - user_mean_pred) ** 2\n",
    "MSE /= rtest.shape[0]\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0626857671311813"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using anime_mean\n",
    "MSE = 0\n",
    "for row in rtest.itertuples():\n",
    "    try:\n",
    "        anime_mean_pred = anime_mean[row.anime_id]\n",
    "    except KeyError:\n",
    "        anime_mean_pred = all_mean\n",
    "    MSE += (row.rating - anime_mean_pred) ** 2\n",
    "MSE /= rtest.shape[0]\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.472705125578593"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using all_mean\n",
    "MSE = 0\n",
    "for row in rtest.itertuples():\n",
    "    MSE += (row.rating - all_mean) ** 2\n",
    "MSE /= rtest.shape[0]\n",
    "MSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "using https://surpriselib.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.3164\n",
      "The predicted rating for user 1 and anime 21 is 8.09\n"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Load your dataframes\n",
    "anime_df = anime\n",
    "rtrain_df = rtrain\n",
    "rtest_df = rtest\n",
    "\n",
    "# Create a Reader object for parsing the ratings dataframes\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "\n",
    "# Load trainset and testset from your pre-split rtrain and rtest dataframes\n",
    "train_data = Dataset.load_from_df(rtrain_df, reader)\n",
    "trainset = train_data.build_full_trainset()\n",
    "\n",
    "test_data = Dataset.load_from_df(rtest_df, reader)\n",
    "testset = test_data.construct_testset(raw_testset=test_data.raw_ratings)\n",
    "\n",
    "# Train the SVD algorithm on the trainset\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Test the algorithm on the testset\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = accuracy.mse(predictions)\n",
    "\n",
    "# Function to predict the rating\n",
    "def predict_rating(user_id, anime_id):\n",
    "    return algo.predict(user_id, anime_id).est\n",
    "\n",
    "# Example usage\n",
    "user_id = 1\n",
    "anime_id = 21 # One Piece\n",
    "predicted_rating = predict_rating(user_id, anime_id)\n",
    "print(f\"The predicted rating for user {user_id} and anime {anime_id} is {predicted_rating:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori Algorithm\n",
    "\n",
    "using https://github.com/rasbt/mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended animes for user 1: []\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Convert user_id, anime_id pairs into lists of animes watched by each user\n",
    "watched_animes = rtrain_df.groupby(\"user_id\")[\"anime_id\"].apply(list)\n",
    "\n",
    "# Use TransactionEncoder to encode the dataset into a one-hot encoded DataFrame\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(watched_animes).transform(watched_animes)\n",
    "watched_animes_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply the Apriori algorithm to find frequent itemsets\n",
    "min_support = 0.05\n",
    "frequent_itemsets = apriori(watched_animes_encoded, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Generate association rules from the frequent itemsets\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "# Function to generate recommendations based on the Apriori algorithm\n",
    "def apriori_recommendations(user_id, top_n=10):\n",
    "    watched = set(rtrain_df[rtrain_df[\"user_id\"] == user_id][\"anime_id\"].tolist())\n",
    "    candidate_rules = rules[rules[\"antecedents\"].apply(lambda x: x.issubset(watched))]\n",
    "    candidate_rules = candidate_rules.sort_values(\"confidence\", ascending=False)\n",
    "    \n",
    "    recommendations = []\n",
    "    for _, row in candidate_rules.iterrows():\n",
    "        new_recommendations = list(row[\"consequents\"] - watched)\n",
    "        recommendations.extend(new_recommendations)\n",
    "        if len(recommendations) >= top_n:\n",
    "            break\n",
    "    \n",
    "    return recommendations[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended animes for user 44017: [1195, 1195, 2904, 2904, 2904, 2904, 2904, 2904, 2904, 1535]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_id = 44017\n",
    "recommended_animes = apriori_recommendations(user_id, top_n=10)\n",
    "print(f\"Recommended animes for user {user_id}: {recommended_animes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the generated recommendations will not have predicted ratings, as the Apriori algorithm is not designed for rating prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
