{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BharathSShankar/DSA4212_Assignments/blob/bharath-exp/VOGN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist torchmetrics\n",
        "!python -m medmnist download"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2eVz6yJqYkW",
        "outputId": "f5742679-4fbc-499b-9b88-cbed2be7ef0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-2.2.1-py3-none-any.whl (21 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from medmnist) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from medmnist) (0.15.1+cu118)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from medmnist) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from medmnist) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from medmnist) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from medmnist) (4.65.0)\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->medmnist) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->medmnist) (16.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->medmnist) (2.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->medmnist) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (2023.4.12)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (1.10.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (2.25.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->medmnist) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->medmnist) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->medmnist) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->medmnist) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=dfcbca2588717d60f4544b3dc1bce5d578aa2bd8efe922b6d23f3af1f969a264\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, torchmetrics, medmnist\n",
            "Successfully installed fire-0.5.0 medmnist-2.2.1 torchmetrics-0.11.4\n",
            "Downloading pathmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/pathmnist.npz?download=1 to /root/.medmnist/pathmnist.npz\n",
            "100% 205615438/205615438 [02:16<00:00, 1503680.83it/s]\n",
            "Downloading chestmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/chestmnist.npz?download=1 to /root/.medmnist/chestmnist.npz\n",
            "100% 82802576/82802576 [01:08<00:00, 1211772.74it/s]\n",
            "Downloading dermamnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/dermamnist.npz?download=1 to /root/.medmnist/dermamnist.npz\n",
            "100% 19725078/19725078 [00:36<00:00, 537852.45it/s]\n",
            "Downloading octmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/octmnist.npz?download=1 to /root/.medmnist/octmnist.npz\n",
            "100% 54938180/54938180 [01:44<00:00, 528014.27it/s]\n",
            "Downloading pneumoniamnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/pneumoniamnist.npz?download=1 to /root/.medmnist/pneumoniamnist.npz\n",
            "100% 4170669/4170669 [00:07<00:00, 524409.95it/s]\n",
            "Downloading retinamnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/retinamnist.npz?download=1 to /root/.medmnist/retinamnist.npz\n",
            "100% 3291041/3291041 [00:06<00:00, 523926.80it/s]\n",
            "Downloading breastmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/breastmnist.npz?download=1 to /root/.medmnist/breastmnist.npz\n",
            "100% 559580/559580 [00:01<00:00, 515955.18it/s]\n",
            "Downloading bloodmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/bloodmnist.npz?download=1 to /root/.medmnist/bloodmnist.npz\n",
            "100% 35461855/35461855 [01:05<00:00, 538045.14it/s]\n",
            "Downloading tissuemnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/tissuemnist.npz?download=1 to /root/.medmnist/tissuemnist.npz\n",
            "100% 124962739/124962739 [02:22<00:00, 878396.85it/s] \n",
            "Downloading organamnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/organamnist.npz?download=1 to /root/.medmnist/organamnist.npz\n",
            "100% 38247903/38247903 [01:10<00:00, 540088.45it/s]\n",
            "Downloading organcmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/organcmnist.npz?download=1 to /root/.medmnist/organcmnist.npz\n",
            "100% 15527535/15527535 [00:28<00:00, 536121.92it/s]\n",
            "Downloading organsmnist...\n",
            "Downloading https://zenodo.org/record/6496656/files/organsmnist.npz?download=1 to /root/.medmnist/organsmnist.npz\n",
            "100% 16528536/16528536 [00:31<00:00, 531278.17it/s]\n",
            "Downloading organmnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/organmnist3d.npz?download=1 to /root/.medmnist/organmnist3d.npz\n",
            "100% 32657407/32657407 [00:34<00:00, 936444.22it/s] \n",
            "Downloading nodulemnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/nodulemnist3d.npz?download=1 to /root/.medmnist/nodulemnist3d.npz\n",
            "100% 29299364/29299364 [00:43<00:00, 678491.98it/s] \n",
            "Downloading adrenalmnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/adrenalmnist3d.npz?download=1 to /root/.medmnist/adrenalmnist3d.npz\n",
            "100% 276833/276833 [00:00<00:00, 573718.99it/s]\n",
            "Downloading fracturemnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/fracturemnist3d.npz?download=1 to /root/.medmnist/fracturemnist3d.npz\n",
            "100% 3278419/3278419 [00:06<00:00, 517586.56it/s]\n",
            "Downloading vesselmnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/vesselmnist3d.npz?download=1 to /root/.medmnist/vesselmnist3d.npz\n",
            "100% 398373/398373 [00:00<00:00, 550345.71it/s]\n",
            "Downloading synapsemnist3d...\n",
            "Downloading https://zenodo.org/record/6496656/files/synapsemnist3d.npz?download=1 to /root/.medmnist/synapsemnist3d.npz\n",
            "100% 38034583/38034583 [01:10<00:00, 541579.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import Optimizer\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "import copy"
      ],
      "metadata": {
        "id": "EzHf-HnBqP5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/DSA4212/Assignment 3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHc9K7AgqP0Y",
        "outputId": "09c7e0d5-4f2b-4e2f-faf3-390cc87fe5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/DSA4212/Assignment 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import medmnist\n",
        "from medmnist import INFO, Evaluator"
      ],
      "metadata": {
        "id": "HmHTuqmGqPsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import copy\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "class VOGN(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, weight_decay=0, delta=25 / 71800, tau=1, gamma=5e-2):\n",
        "        defaults = dict(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay, delta=delta, tau=tau, gamma=gamma)\n",
        "        super(VOGN, self).__init__(params, defaults)\n",
        "\n",
        "        # Initialize m and mu to zero and a deepcopy of the initial parameters respectively\n",
        "        self.m = [torch.zeros_like(p) for p in self.param_groups[0]['params']]\n",
        "        self.mu = copy.deepcopy(self.param_groups[0]['params'])\n",
        "        self.s = None\n",
        "\n",
        "    def _mean_grad_hessian_approx(self, loss):\n",
        "        all_grads = torch.autograd.grad(loss, self.param_groups[0]['params'], create_graph=True)\n",
        "        grads_squared = [g * g for g in all_grads]\n",
        "        return all_grads, grads_squared\n",
        "\n",
        "    def _vogn_step(self, gh):\n",
        "        # Update m\n",
        "        self.m = [m * self.param_groups[0][\"beta_1\"] + (g + mu * self.param_groups[0][\"delta\"]) * (1 - self.param_groups[0][\"beta_1\"]) for m, g, mu in zip(self.m, gh[0], self.mu)]\n",
        "        self.m = [m.detach() for m in self.m]\n",
        "\n",
        "        # Update s\n",
        "        self.s = [s * (1 - self.param_groups[0][\"beta_2\"] * self.param_groups[0][\"tau\"]) + h * self.param_groups[0][\"beta_2\"] * self.param_groups[0][\"tau\"] if s is not None else h * self.param_groups[0][\"beta_2\"] * self.param_groups[0][\"tau\"] for s, h in zip(self.s, gh[1])]\n",
        "        self.s = [s.detach() for s in self.s]\n",
        "\n",
        "        # Update mu\n",
        "        self.mu = [mu - m * self.param_groups[0][\"lr\"] / (s + self.param_groups[0][\"delta\"] + self.param_groups[0][\"gamma\"]) for mu, m, s in zip(self.mu, self.m, self.s)]\n",
        "        self.mu = [mu.detach() for mu in self.mu]\n",
        "\n",
        "    def step(self, losses):\n",
        "        # Calculate current gradient and hessian approximation\n",
        "        if not self.s:\n",
        "            _, self.s = self._mean_grad_hessian_approx(torch.sum(losses))\n",
        "\n",
        "        total_loss = losses.mean()\n",
        "        all_grads, grads_squared = self._mean_grad_hessian_approx(total_loss)\n",
        "        self._vogn_step((all_grads, grads_squared))\n",
        "\n",
        "        # Update model parameters with new mean value\n",
        "        with torch.no_grad():\n",
        "            for p, mu in zip(self.param_groups[0][\"params\"], self.mu):\n",
        "                p.copy_(mu)"
      ],
      "metadata": {
        "id": "7YA2WZwGGzUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_flag = 'pathmnist'\n",
        "# data_flag = 'breastmnist'\n",
        "download = True\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 128\n",
        "lr = 0.001\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])\n"
      ],
      "metadata": {
        "id": "iX5klY-OplsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
        "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
        "\n",
        "pil_dataset = DataClass(split='train', download=download)\n",
        "\n",
        "# encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "JFq-QFMOplop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0463bcd8-0b79-47b2-bcd2-2fb1ed60e8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PI = 0.5\n",
        "SIGMA_1 = torch.FloatTensor([np.exp(1)]).to(DEVICE)\n",
        "SIGMA_2 = torch.FloatTensor([np.exp(-4)]).to(DEVICE)"
      ],
      "metadata": {
        "id": "Yxeagghtql19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Gaussian(object):\n",
        "    def __init__(self, mu, rho):\n",
        "        super().__init__()\n",
        "        self.mu = mu\n",
        "        self.rho = rho\n",
        "        self.normal = torch.distributions.Normal(0,1)\n",
        "    \n",
        "    @property\n",
        "    def sigma(self):\n",
        "        return torch.log1p(torch.exp(self.rho))\n",
        "    \n",
        "    def sample(self):\n",
        "        epsilon = self.normal.sample(self.rho.size()).to(DEVICE)\n",
        "        return self.mu + self.sigma * epsilon\n",
        "    \n",
        "    def log_prob(self, input):\n",
        "        return (-np.log(np.sqrt(2 * np.pi))\n",
        "                - torch.log(self.sigma)\n",
        "                - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"
      ],
      "metadata": {
        "id": "w8aEu8AYqlzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaleMixtureGaussian(object):\n",
        "    def __init__(self, pi, sigma1, sigma2):\n",
        "        super().__init__()\n",
        "        self.pi = pi\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        self.gaussian1 = torch.distributions.Normal(0,sigma1)\n",
        "        self.gaussian2 = torch.distributions.Normal(0,sigma2)\n",
        "    \n",
        "    def log_prob(self, input):\n",
        "        prob1 = torch.exp(self.gaussian1.log_prob(input))\n",
        "        prob2 = torch.exp(self.gaussian2.log_prob(input))\n",
        "        return (torch.log(self.pi * prob1 + (1-self.pi) * prob2)).sum()"
      ],
      "metadata": {
        "id": "tsxgHJMOqlwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianDense(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.1, 0.1))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-7,-5))\n",
        "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.1, 0.1))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-7,-5))\n",
        "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
        "\n",
        "        self.weight_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.bias_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.log_prior = 0\n",
        "        self.log_variational_posterior = 0\n",
        "\n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        if self.training or sample:\n",
        "            weight = self.weight.sample()\n",
        "            bias = self.bias.sample()\n",
        "        else:\n",
        "            weight = self.weight.mu\n",
        "            bias = self.bias.mu\n",
        "        if self.training or calculate_log_probs:\n",
        "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
        "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
        "        else:\n",
        "            self.log_prior, self.log_variational_posterior = 0, 0\n",
        "\n",
        "        return F.linear(input, weight, bias)\n",
        "    \n",
        "    def kl_loss(self):\n",
        "        kl = self.weight_prior.log_prob(self.weight.mu) - self.weight.log_prob(self.weight.mu)\n",
        "        kl += self.bias_prior.log_prob(self.bias.mu) - self.bias.log_prob(self.bias.mu)\n",
        "        return kl.sum()"
      ],
      "metadata": {
        "id": "uLiPJsrmqltd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, k_size):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.k_size = k_size\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_channels, in_channels, k_size, k_size).uniform_(-0.1, 0.1))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_channels, in_channels, k_size,k_size).uniform_(-7,-5))\n",
        "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_channels).uniform_(-0.1, 0.1))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_channels).uniform_(-7,-5))\n",
        "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
        "\n",
        "        self.weight_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.bias_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.log_prior = 0\n",
        "        self.log_variational_posterior = 0\n",
        "\n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        if self.training or sample:\n",
        "            weight = self.weight.sample()\n",
        "            bias = self.bias.sample()\n",
        "        else:\n",
        "            weight = self.weight.mu\n",
        "            bias = self.bias.mu\n",
        "        if self.training or calculate_log_probs:\n",
        "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
        "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
        "        else:\n",
        "            self.log_prior, self.log_variational_posterior = 0, 0\n",
        "\n",
        "        return F.conv2d(input, weight, bias)\n",
        "    \n",
        "    def kl_loss(self):\n",
        "        kl = self.weight_prior.log_prob(self.weight.mu) - self.weight.log_prob(self.weight.mu)\n",
        "        kl += self.bias_prior.log_prob(self.bias.mu) - self.bias.log_prob(self.bias.mu)\n",
        "        return kl.sum()"
      ],
      "metadata": {
        "id": "K2jeI5kvqlql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bayesian_net(net, train_loader, test_loader, n_epochs=20, lr=1e-2, log_interval=10, beta=1e-7, k=5):\n",
        "    # Define loss function and optimizer\n",
        "    loss_func = nn.CrossEntropyLoss(reduce = False)\n",
        "\n",
        "    # Move model to device\n",
        "    net.to(DEVICE)\n",
        "\n",
        "    # Initialize VOGN\n",
        "    vogn = VOGN(net.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training mode\n",
        "        net.train()\n",
        "        train_loss = 0\n",
        "        ce_loss = 0\n",
        "        kl_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}\") as t:\n",
        "            for batch_idx, (data, target) in enumerate(t):\n",
        "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "                # Compute loss and update model\n",
        "                losses = []\n",
        "                for i in range(k):\n",
        "                    outputs = net(data)\n",
        "                    ce_loss = loss_func(outputs, target.T[0])\n",
        "                    kl_loss = net.kl_loss() / len(train_loader)\n",
        "                    loss = ce_loss - beta * kl_loss\n",
        "                    for loss_elem in loss:\n",
        "                        loss_elem.backward(retain_graph=True)\n",
        "                    losses.append(loss)\n",
        "                    \n",
        "                losses = torch.cat(losses)\n",
        "                    \n",
        "                vogn.step(losses)\n",
        "                del losses\n",
        "\n",
        "                # Update training statistics\n",
        "                train_loss += loss.mean().item()\n",
        "                ce_loss = ce_loss.mean()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += data.shape[0]\n",
        "                correct += predicted.eq(target.T[0]).sum().item()\n",
        "\n",
        "                # Log training progress\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    train_acc = correct / total\n",
        "                    train_loss /= log_interval\n",
        "                    ce_loss /= log_interval\n",
        "                    kl_loss /= log_interval\n",
        "                    t.set_postfix(ce_loss=f\"{ce_loss:.6f}\", kl_loss=f\"{kl_loss:.6f}\", loss=f\"{train_loss:.6f}\", accuracy=f\"{train_acc:.2f}\")\n",
        "                    train_loss = 0\n",
        "                    ce_loss = 0\n",
        "                    kl_loss = 0\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "\n",
        "        # Evaluation mode\n",
        "        net.eval()\n",
        "        test_loss = 0\n",
        "        ce_loss = 0\n",
        "        kl_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = net(data)\n",
        "\n",
        "                # Compute loss and update evaluation statistics\n",
        "                ce_loss = loss_func(outputs, target.T[0])\n",
        "                kl_loss = net.kl_loss() / len(test_loader)\n",
        "                loss = ce_loss - beta * kl_loss\n",
        "\n",
        "                test_loss += loss.mean().item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += data.shape[0]\n",
        "                correct += predicted.eq(target.T[0]).sum().item()\n",
        "\n",
        "        # Log evaluation statistics\n",
        "        test_acc = 100. * correct / total\n",
        "        test_loss /= len(test_loader)\n",
        "        print('Test set: Average loss: {:.4f}, CE loss: {:.4f}, KL loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n",
        "            test_loss, ce_loss.mean(), kl_loss, test_acc))\n"
      ],
      "metadata": {
        "id": "JcwlYe-Vqlnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianNeuralNetConv(nn.Module):\n",
        "    def __init__(self, channel_list, input_channels, n_classes):\n",
        "        super().__init__()\n",
        "        self.inputLayer = BayesianConv2D(input_channels, channel_list[0], k_size=3)\n",
        "        layer_list = []\n",
        "        for i in range(1, len(channel_list)):\n",
        "            layer_list.append(BayesianConv2D(channel_list[i - 1], channel_list[i], k_size=1))\n",
        "        self.convs = nn.ModuleList(layer_list)\n",
        "        self.fc = BayesianDense(channel_list[-1] * 9, n_classes)\n",
        "    \n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        x = self.inputLayer(input, sample, calculate_log_probs)\n",
        "        x = F.rrelu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, sample, calculate_log_probs)\n",
        "            x = F.rrelu(x)\n",
        "            x = F.max_pool2d(x, 2)\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.fc(x, sample, calculate_log_probs)\n",
        "        return x\n",
        "    \n",
        "    def kl_loss(self):\n",
        "        kl = 0.0\n",
        "        for layer in self.convs:\n",
        "            kl += layer.kl_loss()\n",
        "        kl += self.inputLayer.kl_loss()\n",
        "        kl += self.fc.kl_loss()\n",
        "        return kl\n",
        "\n"
      ],
      "metadata": {
        "id": "dw30vflSqlkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = BayesianNeuralNetConv([128, 256, 512], 3, 9)"
      ],
      "metadata": {
        "id": "jcC64KuBqStx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_bayesian_net(net, train_loader, test_loader)"
      ],
      "metadata": {
        "id": "aaE7uPp7q2e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "# Define the device\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Define the batch size for the DataLoader\n",
        "targets = []\n",
        "probs = []\n",
        "\n",
        "for images, labels in tqdm(test_loader):\n",
        "    # Move the data to the device\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Get the predicted probabilities from the model\n",
        "    logits = net(images)\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "    # Append the targets and predicted probabilities to the lists.squeeze())\n",
        "    targets.append(labels.detach())\n",
        "    probs.append(probabilities.detach())\n",
        "\n",
        "# Concatenate the targets and predicted probabilities\n",
        "targets = torch.cat(targets).flatten()\n",
        "probs = torch.cat(probs)\n",
        "\n",
        "\n",
        "# Calculate the ECE\n",
        "ece = torchmetrics.functional.classification.multiclass_calibration_error(probs, targets, num_classes=9)\n",
        "print(ece)"
      ],
      "metadata": {
        "id": "5-qYobsgry41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d782c3-9be1-4c4c-8461-b724d8982041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:01<00:00, 16.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0975, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}