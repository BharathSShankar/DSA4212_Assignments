{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BharathSShankar/DSA4212_Assignments/blob/bharath-exp/VOGN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist torchmetrics\n",
        "!python -m medmnist download"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2eVz6yJqYkW",
        "outputId": "20a2db4d-a3c6-404b-f171-1d4352babc79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: medmnist in /usr/local/lib/python3.9/dist-packages (2.2.1)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from medmnist) (0.15.1+cu118)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from medmnist) (8.4.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.9/dist-packages (from medmnist) (0.5.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from medmnist) (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from medmnist) (1.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from medmnist) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from medmnist) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->medmnist) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->medmnist) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->medmnist) (3.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->medmnist) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->medmnist) (2022.7.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (2023.4.12)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (2.25.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (1.10.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->medmnist) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->medmnist) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->medmnist) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->medmnist) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->medmnist) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->medmnist) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Downloading pathmnist...\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Downloading chestmnist...\n",
            "Using downloaded and verified file: /root/.medmnist/chestmnist.npz\n",
            "Downloading dermamnist...\n",
            "Using downloaded and verified file: /root/.medmnist/dermamnist.npz\n",
            "Downloading octmnist...\n",
            "Using downloaded and verified file: /root/.medmnist/octmnist.npz\n",
            "Downloading pneumoniamnist...\n",
            "Using downloaded and verified file: /root/.medmnist/pneumoniamnist.npz\n",
            "Downloading retinamnist...\n",
            "Using downloaded and verified file: /root/.medmnist/retinamnist.npz\n",
            "Downloading breastmnist...\n",
            "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n",
            "Downloading bloodmnist...\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Downloading tissuemnist...\n",
            "Using downloaded and verified file: /root/.medmnist/tissuemnist.npz\n",
            "Downloading organamnist...\n",
            "Using downloaded and verified file: /root/.medmnist/organamnist.npz\n",
            "Downloading organcmnist...\n",
            "Using downloaded and verified file: /root/.medmnist/organcmnist.npz\n",
            "Downloading organsmnist...\n",
            "Using downloaded and verified file: /root/.medmnist/organsmnist.npz\n",
            "Downloading organmnist3d...\n",
            "Using downloaded and verified file: /root/.medmnist/organmnist3d.npz\n",
            "Downloading nodulemnist3d...\n",
            "Using downloaded and verified file: /root/.medmnist/nodulemnist3d.npz\n",
            "Downloading adrenalmnist3d...\n",
            "Using downloaded and verified file: /root/.medmnist/adrenalmnist3d.npz\n",
            "Downloading fracturemnist3d...\n",
            "Using downloaded and verified file: /root/.medmnist/fracturemnist3d.npz\n",
            "Downloading vesselmnist3d...\n",
            "Using downloaded and verified file: /root/.medmnist/vesselmnist3d.npz\n",
            "Downloading synapsemnist3d...\n",
            "Using downloaded and verified file: /root/.medmnist/synapsemnist3d.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import Optimizer\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "import copy"
      ],
      "metadata": {
        "id": "EzHf-HnBqP5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/DSA4212/Assignment 3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHc9K7AgqP0Y",
        "outputId": "95029113-dd59-41f1-9c65-74444d5a5da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/DSA4212/Assignment 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import medmnist\n",
        "from medmnist import INFO, Evaluator"
      ],
      "metadata": {
        "id": "HmHTuqmGqPsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class VOGN(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, weight_decay=0, delta = 25 / 71800, tau = 1, gamma = 5e-2):\n",
        "        defaults = dict(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, weight_decay=weight_decay, delta = delta, tau = tau, gamma = gamma)\n",
        "        super(VOGN, self).__init__(params, defaults)\n",
        "\n",
        "        # Initialize m and mu to zero and a deepcopy of the initial parameters respectively\n",
        "        self.m = [torch.zeros_like(p) for p in self.param_groups[0]['params']]\n",
        "        self.mu = copy.deepcopy(self.param_groups[0]['params'])\n",
        "        self.s = None\n",
        "        self.net_params = self.param_groups[0]['params']\n",
        "\n",
        "    def _squared_grad_hessian_approx(self, loss):\n",
        "        grads = [torch.autograd.grad(loss_elem, self.net_params, retain_graph=True) for loss_elem in loss]\n",
        "        grads_squared = [[g * g for g in grad] for grad in grads]\n",
        "        mean_grad = []\n",
        "        grad_hess = []\n",
        "        for i in range(len(grads[0])):\n",
        "            grad_param = torch.cat([grad[i] for grad in grads]).mean(axis = 0)\n",
        "            hess_param = torch.cat([grad[i] for grad in grads_squared]).mean(axis = 0)\n",
        "            mean_grad.append(grad_param)\n",
        "            grad_hess.append(hess_param)\n",
        "        return grad_hess, mean_grad\n",
        "\n",
        "    def _vari_gradhess(self, loss):\n",
        "        \"\"\" computes gradient and hessian approximation averaged based on current variational distribution \"\"\"\n",
        "        vari_grads = []\n",
        "        vari_hess = []\n",
        "        for i, loss_elems in enumerate(loss):\n",
        "            grad_hess, mean_grad = self._squared_grad_hessian_approx(loss_elems)\n",
        "            vari_grads.append(mean_grad)\n",
        "            vari_hess.append(grad_hess)\n",
        "        out_grads = []\n",
        "        out_hess = []\n",
        "        for i in range(len(mean_grad[0])):\n",
        "            out_grads.append(torch.cat([vari_grad[i].unsqueeze(0) for vari_grad in vari_grads]).mean(axis = 0).squeeze())\n",
        "            out_hess.append(torch.cat([vari_hes[i].unsqueeze(0) for vari_hes in vari_hess]).mean(axis = 0).squeeze())\n",
        "        return out_grads, out_hess\n",
        "    \n",
        "    def _vogn_step(self, gh):\n",
        "        # update m\n",
        "        for i, (g, mu) in enumerate(zip(gh[0], self.mu)):\n",
        "            self.m[i] = self.m[i] * self.param_groups[0][\"beta_1\"] + (g + mu * self.param_groups[0][\"delta\"]) * (1 - self.param_groups[0][\"beta_1\"])\n",
        "\n",
        "        # update s\n",
        "        for i, h in enumerate(gh[1]):\n",
        "            self.s[i] = self.s[i] * (1 - self.param_groups[0][\"beta_2\"] * self.param_groups[0][\"tau\"]) + h * self.param_groups[0][\"beta_2\"] * self.param_groups[0][\"tau\"]\n",
        "\n",
        "        # update mu\n",
        "        for i, (m, s) in enumerate(zip(self.m, self.s)):\n",
        "            self.mu[i] = self.mu[i] - m * self.param_groups[0][\"lr\"] / (s + self.param_groups[0][\"delta\"] + self.param_groups[0][\"gamma\"])\n",
        "\n",
        "    def step(self, losses):\n",
        "        # Calculate current gradient and hessian approximation\n",
        "        if not self.s:\n",
        "            self.s, _ = self._squared_grad_hessian_approx(losses[0])\n",
        "        gh = self._vari_gradhess(losses)\n",
        "\n",
        "        # Update VOGN state\n",
        "        self._vogn_step(gh)\n",
        "\n",
        "        # Update model parameters with new mean value\n",
        "        with torch.no_grad():\n",
        "            for p, mu in zip(self.net_params, self.mu):\n",
        "                p.copy_(mu)\n"
      ],
      "metadata": {
        "id": "7YA2WZwGGzUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_flag = 'pathmnist'\n",
        "# data_flag = 'breastmnist'\n",
        "download = True\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 128\n",
        "lr = 0.001\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])\n"
      ],
      "metadata": {
        "id": "iX5klY-OplsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
        "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
        "\n",
        "pil_dataset = DataClass(split='train', download=download)\n",
        "\n",
        "# encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "JFq-QFMOplop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5aae2c5-20dd-456d-d0bf-48ca4df908a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PI = 0.5\n",
        "SIGMA_1 = torch.FloatTensor([np.exp(1)]).to(DEVICE)\n",
        "SIGMA_2 = torch.FloatTensor([np.exp(-4)]).to(DEVICE)"
      ],
      "metadata": {
        "id": "Yxeagghtql19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Gaussian(object):\n",
        "    def __init__(self, mu, rho):\n",
        "        super().__init__()\n",
        "        self.mu = mu\n",
        "        self.rho = rho\n",
        "        self.normal = torch.distributions.Normal(0,1)\n",
        "    \n",
        "    @property\n",
        "    def sigma(self):\n",
        "        return torch.log1p(torch.exp(self.rho))\n",
        "    \n",
        "    def sample(self):\n",
        "        epsilon = self.normal.sample(self.rho.size()).to(DEVICE)\n",
        "        return self.mu + self.sigma * epsilon\n",
        "    \n",
        "    def log_prob(self, input):\n",
        "        return (-np.log(np.sqrt(2 * np.pi))\n",
        "                - torch.log(self.sigma)\n",
        "                - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"
      ],
      "metadata": {
        "id": "w8aEu8AYqlzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaleMixtureGaussian(object):\n",
        "    def __init__(self, pi, sigma1, sigma2):\n",
        "        super().__init__()\n",
        "        self.pi = pi\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        self.gaussian1 = torch.distributions.Normal(0,sigma1)\n",
        "        self.gaussian2 = torch.distributions.Normal(0,sigma2)\n",
        "    \n",
        "    def log_prob(self, input):\n",
        "        prob1 = torch.exp(self.gaussian1.log_prob(input))\n",
        "        prob2 = torch.exp(self.gaussian2.log_prob(input))\n",
        "        return (torch.log(self.pi * prob1 + (1-self.pi) * prob2)).sum()"
      ],
      "metadata": {
        "id": "tsxgHJMOqlwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianDense(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.1, 0.1))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-7,-5))\n",
        "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.1, 0.1))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-7,-5))\n",
        "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
        "\n",
        "        self.weight_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.bias_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.log_prior = 0\n",
        "        self.log_variational_posterior = 0\n",
        "\n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        if self.training or sample:\n",
        "            weight = self.weight.sample()\n",
        "            bias = self.bias.sample()\n",
        "        else:\n",
        "            weight = self.weight.mu\n",
        "            bias = self.bias.mu\n",
        "        if self.training or calculate_log_probs:\n",
        "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
        "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
        "        else:\n",
        "            self.log_prior, self.log_variational_posterior = 0, 0\n",
        "\n",
        "        return F.linear(input, weight, bias)\n",
        "    \n",
        "    def kl_loss(self):\n",
        "        kl = self.weight_prior.log_prob(self.weight.mu) - self.weight.log_prob(self.weight.mu)\n",
        "        kl += self.bias_prior.log_prob(self.bias.mu) - self.bias.log_prob(self.bias.mu)\n",
        "        return kl.sum()"
      ],
      "metadata": {
        "id": "uLiPJsrmqltd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, k_size):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.k_size = k_size\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_channels, in_channels, k_size, k_size).uniform_(-0.1, 0.1))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_channels, in_channels, k_size,k_size).uniform_(-7,-5))\n",
        "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_channels).uniform_(-0.1, 0.1))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_channels).uniform_(-7,-5))\n",
        "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
        "\n",
        "        self.weight_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.bias_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
        "        self.log_prior = 0\n",
        "        self.log_variational_posterior = 0\n",
        "\n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        if self.training or sample:\n",
        "            weight = self.weight.sample()\n",
        "            bias = self.bias.sample()\n",
        "        else:\n",
        "            weight = self.weight.mu\n",
        "            bias = self.bias.mu\n",
        "        if self.training or calculate_log_probs:\n",
        "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
        "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
        "        else:\n",
        "            self.log_prior, self.log_variational_posterior = 0, 0\n",
        "\n",
        "        return F.conv2d(input, weight, bias)\n",
        "    \n",
        "    def kl_loss(self):\n",
        "        kl = self.weight_prior.log_prob(self.weight.mu) - self.weight.log_prob(self.weight.mu)\n",
        "        kl += self.bias_prior.log_prob(self.bias.mu) - self.bias.log_prob(self.bias.mu)\n",
        "        return kl.sum()"
      ],
      "metadata": {
        "id": "K2jeI5kvqlql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bayesian_net(net, train_loader, test_loader, n_epochs=20, lr=3e-4, log_interval=10, beta=1e-7, k=5):\n",
        "    # Define loss function and optimizer\n",
        "    loss_func = nn.CrossEntropyLoss(reduce = False)\n",
        "\n",
        "    # Move model to device\n",
        "    net.to(DEVICE)\n",
        "\n",
        "    # Initialize VOGN\n",
        "    vogn = VOGN(net.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training mode\n",
        "        net.train()\n",
        "        train_loss = 0\n",
        "        ce_loss = 0\n",
        "        kl_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}\") as t:\n",
        "            for batch_idx, (data, target) in enumerate(t):\n",
        "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "                # Compute loss and update model\n",
        "                losses = []\n",
        "                for i in range(k):\n",
        "                    outputs = net(data)\n",
        "                    ce_loss = loss_func(outputs, target.T[0])\n",
        "                    kl_loss = net.kl_loss() / len(train_loader)\n",
        "                    loss = ce_loss - beta * kl_loss\n",
        "                    for loss_elem in loss:\n",
        "                        loss_elem.backward(retain_graph=True)\n",
        "                    losses.append(loss)\n",
        "\n",
        "                vogn.step(losses)\n",
        "\n",
        "                # Update training statistics\n",
        "                train_loss += loss.mean().item()\n",
        "                ce_loss = ce_loss.mean()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += data.shape[0]\n",
        "                correct += predicted.eq(target.T[0]).sum().item()\n",
        "\n",
        "                # Log training progress\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    train_acc = correct / total\n",
        "                    train_loss /= log_interval\n",
        "                    ce_loss /= log_interval\n",
        "                    kl_loss /= log_interval\n",
        "                    t.set_postfix(ce_loss=f\"{ce_loss:.6f}\", kl_loss=f\"{kl_loss:.6f}\", loss=f\"{train_loss:.6f}\", accuracy=f\"{train_acc:.2f}\")\n",
        "                    train_loss = 0\n",
        "                    ce_loss = 0\n",
        "                    kl_loss = 0\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "\n",
        "        # Evaluation mode\n",
        "        net.eval()\n",
        "        test_loss = 0\n",
        "        ce_loss = 0\n",
        "        kl_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = net(data)\n",
        "\n",
        "                # Compute loss and update evaluation statistics\n",
        "                ce_loss = loss_func(outputs, target.T[0])\n",
        "                kl_loss = net.kl_loss() / len(test_loader)\n",
        "                loss = ce_loss - beta * kl_loss\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += data.shape[0]\n",
        "                correct += predicted.eq(target.T[0]).sum().item()\n",
        "\n",
        "        # Log evaluation statistics\n",
        "        test_acc = 100. * correct / total\n",
        "        test_loss /= len(test_loader)\n",
        "        print('Test set: Average loss: {:.4f}, CE loss: {:.4f}, KL loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n",
        "            test_loss, ce_loss, kl_loss, test_acc))\n"
      ],
      "metadata": {
        "id": "JcwlYe-Vqlnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianNeuralNetConv(nn.Module):\n",
        "    def __init__(self, channel_list, input_channels, n_classes):\n",
        "        super().__init__()\n",
        "        self.inputLayer = BayesianConv2D(input_channels, channel_list[0], k_size=3)\n",
        "        layer_list = []\n",
        "        for i in range(1, len(channel_list)):\n",
        "            layer_list.append(BayesianConv2D(channel_list[i - 1], channel_list[i], k_size=1))\n",
        "        self.convs = nn.ModuleList(layer_list)\n",
        "        self.fc = BayesianDense(channel_list[-1] * 9, n_classes)\n",
        "    \n",
        "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
        "        x = self.inputLayer(input, sample, calculate_log_probs)\n",
        "        x = F.rrelu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, sample, calculate_log_probs)\n",
        "            x = F.rrelu(x)\n",
        "            x = F.max_pool2d(x, 2)\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.fc(x, sample, calculate_log_probs)\n",
        "        return x\n",
        "    \n",
        "    def kl_loss(self):\n",
        "        kl = 0.0\n",
        "        for layer in self.convs:\n",
        "            kl += layer.kl_loss()\n",
        "        kl += self.inputLayer.kl_loss()\n",
        "        kl += self.fc.kl_loss()\n",
        "        return kl\n",
        "\n"
      ],
      "metadata": {
        "id": "dw30vflSqlkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = BayesianNeuralNetConv([128, 256, 512], 3, 9)\n",
        "\n",
        "train_bayesian_net(net, train_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaE7uPp7q2e9",
        "outputId": "056b8a22-5d2a-4836-dd03-78e6e2ad459a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "Epoch 1:   1%|          | 5/704 [01:04<2:26:28, 12.57s/it, accuracy=0.08, ce_loss=0.225333, kl_loss=-168.958572, loss=0.225350]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5-qYobsgry41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}